# **Scientific Programming ‚Äì Lab 5**

## Training Custom Language Models with Tinker

### **Session Overview**

This lab introduces students to advanced LLM fine-tuning workflows, model ownership strategies, and practical approaches to building specialized AI systems. Students learn to:

* Understand synthetic data generation and tool-using agents
* Differentiate between structured RAG and traditional embedding-based retrieval
* Use the Tinker training API for fine-tuning open-weight models
* Compare pre-training vs fine-tuning approaches
* Apply parameter-efficient fine-tuning (LoRA) techniques
* Implement reinforcement learning from human feedback (RLHF)
* Evaluate vendor risks and build sustainable AI architectures
* Deploy and manage custom models for production use

**Real-world applications:**

* Building domain-specific AI assistants (legal, medical, technical)
* Creating cost-effective alternatives to frontier models
* Implementing tool-using agents with structured data
* Developing proprietary models to reduce vendor dependency
* Training specialized code generators and problem solvers

---

## **Learning Objectives**

By the end of this lab, you should be able to:

1. Understand the architecture of tool-using agents and structured RAG systems
2. Generate and use synthetic datasets for model training
3. Configure and use the Tinker API for model fine-tuning
4. Choose appropriate models and training strategies for different use cases
5. Implement LoRA-based parameter-efficient fine-tuning
6. Design reinforcement learning workflows with reward functions
7. Evaluate trade-offs between system prompts and fine-tuning
8. Assess vendor risks and plan for model ownership strategies
9. Deploy and iterate on custom-trained models
10. Use the Tinker Cookbook for common training recipes

---

## **Prerequisites**

* Completion of Lab 05 Tinker Presentation (fine-tuning fundamentals)
* Understanding of LLM basics and inference
* Python programming experience
* Familiarity with API-based workflows
* Basic understanding of machine learning concepts
* Access to Tinker API credentials (apply at thinking-machines.ai)

---

## **1. Synthetic Data and Tool-Using Agents**

### **The Synthetic Data Revolution**

Modern LLM training increasingly relies on **synthetic data** ‚Äì data generated by AI models rather than humans. This approach was successfully demonstrated by Pleias AI with their **Baguettotron** model.

**Key insight:** You can use large frontier models (GPT-5, Claude, etc.) to generate training data for smaller, specialized models.

**Example workflow:**
```python
# 1. Use frontier model to generate training data
synthetic_examples = []
for topic in domain_topics:
    prompt = f"Generate 10 expert-level Q&A pairs about {topic}"
    examples = gpt5.generate(prompt)
    synthetic_examples.extend(examples)

# 2. Train small model on synthetic data
small_model.train(synthetic_examples)

# 3. Deploy small model (cheaper, faster, owned by you)
```

### **Tool-Using Agents: TypeAgent Example**

Guido van Rossum (creator of Python) and his team at Microsoft developed **TypeAgent**, demonstrating the power of structured tool use.

**Architecture:**

```
User Query
    ‚Üì
LLM Agent (understands intent)
    ‚Üì
Structured Tool Calls (SQL, API, search)
    ‚Üì
Database / External Systems
    ‚Üì
Retrieved Data
    ‚Üì
LLM Agent (synthesizes answer)
    ‚Üì
Final Response
```

**Key difference from traditional LLMs:**
- Traditional: Everything stored in model weights
- Tool-using: Model **retrieves** fresh data from external sources

**Benefits:**
* Always up-to-date information
* No hallucinations about facts (queries real data)
* Scalable (don't need to retrain for new information)
* Explainable (can trace which data sources were used)

### **Practical Applications**

| Use Case | Traditional LLM | Tool-Using Agent |
|----------|----------------|------------------|
| Customer support | Limited to training data | Queries live ticket system, CRM |
| Code assistance | Suggests from patterns | Searches actual codebase, docs |
| Financial analysis | Generic advice | Fetches real-time market data |
| Medical diagnosis | General knowledge | Queries patient records, research DBs |
| Legal research | Outdated case law | Searches current legal databases |

---

## **2. Structured RAG vs Traditional Embeddings**

### **Classic RAG Architecture**

Traditional retrieval-augmented generation:

1. **Document preparation:**
   ```python
   chunks = split_documents(documents, chunk_size=512)
   embeddings = embed_model.encode(chunks)
   vector_db.store(chunks, embeddings)
   ```

2. **Query time:**
   ```python
   query_embedding = embed_model.encode(user_query)
   relevant_chunks = vector_db.search(query_embedding, k=5)
   response = llm.generate(f"Context: {relevant_chunks}\n\nQuestion: {user_query}")
   ```

**Limitations:**
- Retrieves entire text chunks (verbose, expensive)
- No understanding of structure
- Can retrieve irrelevant context
- Expensive to process large chunks through LLM

### **Structured RAG: The Modern Approach**

Instead of storing full text, extract **structured facts**:

```python
# Traditional: Store full email
email_chunk = """
From: john@company.com
To: team@company.com
Date: 2025-01-15
Subject: Q1 Project Deadline

Hi team,
The Q1 project deadline is March 31st at 5pm.
We need to finalize the design by February 15th.
Please coordinate with Sarah on the frontend components.
"""

# Structured: Extract key facts
structured_facts = {
    "sender": "john@company.com",
    "recipients": ["team@company.com"],
    "date": "2025-01-15",
    "entities": ["Sarah"],
    "deadlines": [
        {"task": "Q1 project", "date": "2025-03-31", "time": "17:00"},
        {"task": "Design finalization", "date": "2025-02-15"}
    ],
    "topics": ["project deadline", "frontend components"],
    "action_items": ["coordinate with Sarah on frontend"]
}
```

### **Benefits of Structured Approach**

**Size reduction:**
```
Full email: ~200 tokens
Structured facts: ~50 tokens
Reduction: 75%
```

**Query efficiency:**
```python
# Traditional: Retrieve full documents, send to LLM
chunks = vector_db.search(query)  # Returns 5 full emails (~1000 tokens)
answer = llm.generate(context=chunks)  # Expensive

# Structured: Query specific facts
facts = fact_db.query("SELECT * FROM deadlines WHERE date < '2025-02-01'")
answer = llm.generate(context=facts)  # Much cheaper
```

### **Example: Email Agent with Structured Data**

**Query:** "Add me to the calendar for the pickle game"

**Traditional RAG:**
1. Embed query
2. Retrieve 5 full email conversations
3. Send all to LLM (~2000 tokens)
4. LLM extracts relevant info
5. Make calendar API call

**Structured RAG:**
1. Query structured fact database:
   ```sql
   SELECT event, date, time, participants
   FROM events
   WHERE event LIKE '%pickle game%'
   ```
2. Check calendar API for conflicts
3. Send compact facts to LLM (~100 tokens)
4. LLM generates calendar update

**Result:**
- 95% reduction in token usage
- 10x faster response time
- More accurate (structured data is explicit)
- Easier to debug (can inspect SQL queries)

---

## **3. Thinking Machines and the Tinker Training API**

### **About Thinking Machines**

**Thinking Machines** is a new AI infrastructure company:

* **Leadership:** Mira Murati (former CTO of OpenAI)
* **Valuation:** Raising at ~$50B
* **Product:** Tinker ‚Äì a training API for researchers and developers

**Core offering:** API-driven model training without managing GPUs

### **What Tinker Provides**

**Training operations:**
- Fine-tuning (supervised learning)
- Reinforcement learning (RL, RLHF)
- Distillation (large model ‚Üí small model)
- Parameter-efficient training (LoRA)

**Key features:**
```python
from tinker import TrainingClient

# Create training session
client = TrainingClient(
    model_name="meta-llama/Llama-3.1-8B",
    lora_config={"rank": 32}
)

# Train
for batch in dataset:
    client.forward_backward(batch, loss_fn="cross_entropy")
    client.optim_step()

# Save checkpoint
checkpoint_uri = client.save_weights_for_sampler()
```

**You handle (CPU work):**
- Data loading and preprocessing
- Training loop logic
- Reward computation (for RL)
- Metrics tracking and logging

**Tinker handles (GPU work):**
- Forward/backward passes
- Gradient computation
- Weight updates
- Distributed training across multiple GPUs
- Checkpoint storage

### **Cost Example**

**Real training run:**
- Cost: ~$3.50
- Data: ~500,000 tokens processed
- Duration: Several hours
- Model: 8B parameter Llama variant

**Compare to self-hosting:**
- GPU rental: $2-5/hour √ó multiple GPUs
- Setup time: days to weeks
- Infrastructure management: ongoing
- Expertise required: CUDA, distributed training, debugging

**Tinker advantage:** No infrastructure management, pay-per-use pricing

---

## **4. Available Models and Pricing**

### **Model Lineup**

Tinker provides access to various **open-weight models**:

| Model Series | Parameter Sizes | Type | Best For |
|-------------|----------------|------|----------|
| **Llama** | 1B, 3B, 8B, 70B, 405B | General purpose | Balanced performance |
| **Qwen** | 1.5B, 7B, 14B, 32B, 72B, 235B | MoE + dense | Cost-effective scaling |
| **DeepSeek** | ~671B | MoE | Maximum capability |

**Model variants:**
- **Base:** Raw pre-trained model (no instruction following)
- **Instruct:** Fine-tuned for chat and instructions
- **MoE (Mixture of Experts):** Efficient large models

### **Understanding Pricing**

**Rate card structure:**
```
Training cost = (compute hours) √ó (model size multiplier) √ó (operation type)

Example rates (simplified):
- 8B model training: ~$2-5/hour
- 70B model training: ~$20-50/hour
- Inference: ~$0.10-1.00 per million tokens
```

**Compare to frontier model APIs:**

| Model | Cost per 1M Output Tokens |
|-------|--------------------------|
| GPT-5.1 | ~$600 |
| Claude Sonnet 4.5 | ~$15 |
| Your trained 8B model | ~$0.10-0.50 |

**Break-even analysis:**
```python
# Cost to train: $100
# Inference savings: $599.90 per 1M tokens

# Break-even point:
tokens_to_break_even = 100 / 599.90 * 1_000_000
# ‚âà 167,000 tokens

# If you process 1M tokens/month:
# Monthly savings: ~$600 - ~$100 (amortized training) = $500
# Annual savings: ~$6,000
```

### **Why New Models Aren't Always "Better"**

**Common misconception:** GPT-5.1 > GPT-4o in all ways

**Reality:** New models are often optimized for **providers**, not users:

‚úÖ **Better for provider:**
- More inference-efficient (cheaper to run)
- Lower power consumption
- Better hardware utilization
- Trained on user data (RLHF feedback)

‚ùì **Not necessarily better for you:**
- May be less capable at specific tasks
- More aligned to general use cases (not your domain)
- More censored/restricted
- API can change or be deprecated

**Example:** GPT-4-turbo vs GPT-4o
- GPT-4o: Faster, cheaper **for OpenAI**
- GPT-4-turbo: Sometimes better at complex reasoning
- Your use case may prefer the older model

---

## **5. Why Train Your Own Model?**

### **The Strategic Case**

**Instead of:** Paying $600/1M tokens to GPT-5.1

**Consider:**

1. **Generate synthetic data** with frontier model
   ```python
   # One-time cost: ~$50-100
   training_data = gpt5.generate_examples(
       domain="customer support",
       n_examples=10000
   )
   ```

2. **Train small model** on that data
   ```python
   # One-time cost: ~$100-500
   my_model = tinker.train(
       base_model="Llama-3.1-8B",
       data=training_data
   )
   ```

3. **Deploy and use** your model
   ```python
   # Ongoing cost: ~$0.50/1M tokens
   response = my_model.generate(user_query)
   ```

### **Benefits of Model Ownership**

**1. Cost reduction**
```
Scenario: 10M tokens/month

Frontier API cost:
- $600/1M √ó 10 = $6,000/month
- $72,000/year

Custom model cost:
- Training: $500 (one-time)
- Inference: $5/month
- $60/year + $500 = $560 first year
- Savings: $71,440 first year
```

**2. Ownership and control**
- ‚úÖ Model is **yours** forever
- ‚úÖ Can't be deprecated or removed
- ‚úÖ No vendor lock-in
- ‚úÖ No API rate limits
- ‚úÖ No usage restrictions

**3. Deployment flexibility**
```
Deployment options:
- Your own GPU (home, office)
- Your own cloud (AWS, GCP, Azure)
- Edge devices (local deployment)
- Custom infrastructure
```

**4. Privacy and compliance**
- Data never leaves your infrastructure
- GDPR/HIPAA compliance easier
- No third-party data sharing
- Full audit trail

**5. Customization**
- Specialized for your exact use case
- Your terminology, format, style
- Domain-specific knowledge
- Reduced hallucinations in your context

### **Risk Mitigation: Vendor Dependency**

**Real risk example:**

> OpenAI releases GPT-6
> ‚Üí Deprecates GPT-5.1 within 6 months
> ‚Üí Your product breaks
> ‚Üí Scramble to update prompts, retune, etc.

**With owned models:**
- Your model continues working
- You upgrade on your timeline
- No forced migrations
- Predictable costs

---

## **6. Pre-Training vs Fine-Tuning**

### **Pre-Training: Building the Foundation**

**What it is:**
- Training a model from scratch on massive corpora
- Billions to trillions of tokens
- Months of compute time
- Millions of dollars in cost

**Data sources:**
```
Pre-training corpus (example):
- Web crawl: 70%
- Books: 10%
- Code: 10%
- Academic papers: 5%
- Other: 5%
Total: ~1-10 trillion tokens
```

**Cost example:**
- Llama 3.1 405B: Estimated $100M-500M
- GPT-4: Estimated $100M+
- Small models (1B-8B): Still $100K-1M

**Output:** A **base model** with broad, general knowledge

**Who does this?**
- Large companies (Meta, Google, OpenAI)
- Research institutions (Mistral, Cohere)
- Consortium efforts (Bloom, EleutherAI)

### **Fine-Tuning: Specialization**

**What it is:**
- Start with existing pre-trained model
- Train on small, focused dataset
- Hours to days of compute
- $100-10,000 typical cost

**Data requirements:**
```
Fine-tuning dataset (example):
- Task-specific examples: 1,000-100,000
- Often <1% of pre-training size
- High quality >> quantity
```

**Goals:**

| Goal | Example | Dataset Size |
|------|---------|--------------|
| Domain adaptation | Legal document analysis | 10,000-50,000 examples |
| Format/style | Chat vs completion | 1,000-10,000 examples |
| Task-specific | Code completion | 5,000-50,000 examples |
| Reduce hallucinations | Medical Q&A | 10,000-100,000 examples |
| Alignment | RLHF for helpfulness | 10,000-1M comparisons |

### **Model Variants Explained**

**Base model:**
```python
# Raw completion, no instruction following
prompt = "The capital of France is"
response = base_model(prompt)
# Output: " Paris. The city is known for..."
# Continues the text, doesn't answer questions
```

**Instruct model:**
```python
# Follows instructions
prompt = "What is the capital of France?"
response = instruct_model(prompt)
# Output: "The capital of France is Paris."
# Understands and responds to the question
```

**Chat model:**
```python
# Multi-turn dialogue
messages = [
    {"role": "user", "content": "What is the capital of France?"},
    {"role": "assistant", "content": "The capital of France is Paris."},
    {"role": "user", "content": "What's the population?"}
]
response = chat_model(messages)
# Output: "Paris has a population of approximately 2.1 million..."
# Maintains context across turns
```

### **Real-World Example: COBOL Refactoring**

**Scenario:** Bank needs to modernize legacy COBOL code

**Approach 1: Use GPT-5 directly**
- ‚ùå GPT-5 knows some COBOL, but not domain-specific patterns
- ‚ùå Costs $600/1M tokens
- ‚ùå May hallucinate incorrect conversions

**Approach 2: Fine-tune specialized model**
```python
# 1. Collect COBOL examples from bank's codebase
cobol_dataset = collect_examples(
    source_language="COBOL",
    target_language="Java",
    domain="banking"
)

# 2. Fine-tune model
cobol_expert = tinker.train(
    base_model="CodeLlama-34B",
    data=cobol_dataset,
    task="code_translation"
)

# 3. Deploy for refactoring
refactored_code = cobol_expert.translate(legacy_cobol)
```

**Results:**
- ‚úÖ Understands bank-specific patterns
- ‚úÖ 10x cheaper at scale
- ‚úÖ Owned by bank (security)
- ‚úÖ Can run on-premises

---

## **7. Full Fine-Tuning vs LoRA**

### **Full Fine-Tuning**

**How it works:**
```python
# Update ALL model parameters
for layer in model.layers:
    for weight in layer.weights:
        weight = weight - learning_rate * gradient
```

**Characteristics:**
- Updates billions of parameters
- Requires storing full model copy
- High memory requirements
- Maximum flexibility and capability

**When to use:**
- Large datasets (100K+ examples)
- Significant domain shift
- Maximum performance needed
- Budget allows

**Costs:**
```
8B model full fine-tuning:
- GPU memory: ~32GB
- Training time: Days
- Cost: $1,000-5,000
```

### **LoRA: Low-Rank Adaptation**

**How it works:**
```python
# Original weight matrix
W = torch.randn(4096, 4096)  # 16M parameters

# LoRA: Add small adapter matrices
A = torch.randn(4096, 32)    # 131K parameters
B = torch.randn(32, 4096)    # 131K parameters

# Forward pass
output = W @ input + (A @ B) @ input

# Only train A and B (keep W frozen)
# Total trainable: 262K instead of 16M (98.4% reduction!)
```

**Visual representation:**
```
Original Model:
[Frozen Weights W] ‚Üí Output

LoRA Model:
[Frozen Weights W] ‚Üí Output
         +
[Trainable A √ó B] ‚Üí Output
```

**Benefits:**

| Aspect | Full FT | LoRA |
|--------|---------|------|
| Parameters updated | 100% | 0.1-1% |
| Memory required | 32-64GB | 8-16GB |
| Training time | Days | Hours |
| Cost | $1,000-5,000 | $100-500 |
| Multiple tasks | Need separate models | Swap adapters |

### **LoRA Configuration**

**Key hyperparameter: Rank**

```python
# Low rank (fast, cheap, less capacity)
lora_config_low = {
    "rank": 8,
    "alpha": 16,
    "dropout": 0.05
}

# Medium rank (balanced)
lora_config_medium = {
    "rank": 32,
    "alpha": 64,
    "dropout": 0.05
}

# High rank (slower, expensive, more capacity)
lora_config_high = {
    "rank": 128,
    "alpha": 256,
    "dropout": 0.05
}
```

**Rule of thumb:**
- Rank 8-16: Simple tasks, small datasets
- Rank 32-64: Most use cases (recommended)
- Rank 128+: Complex domains, large datasets

### **Important: Learning Rate Adjustment**

‚ö†Ô∏è **LoRA requires 20-100x higher learning rates!**

```python
# Full fine-tuning
learning_rate_full = 5e-5

# LoRA (must scale up!)
learning_rate_lora = 5e-5 * 20  # = 1e-3

# Use Tinker's helper
from tinker_cookbook.utils import get_lr
learning_rate = get_lr("meta-llama/Llama-3.1-8B")  # Returns optimal LR
```

**Why?** LoRA adapters are smaller matrices, need larger relative updates to have same effect.

### **LoRA Beyond Text: Image Models**

LoRA is widely used in image generation:

```python
# Stable Diffusion LoRA
from diffusers import StableDiffusionPipeline

# Load base model
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")

# Load LoRA adapters
pipe.load_lora_weights("./anime-style-lora")

# Generate with style
image = pipe("a cat wearing a hat").images[0]
```

**Use cases:**
- Artistic styles (anime, watercolor, etc.)
- Specific characters or objects
- Photography styles
- Architecture and design preferences

---

## **8. Reinforcement Learning and RLHF**

### **Reinforcement Learning Fundamentals**

**Core concepts:**

```python
class RLEnvironment:
    def __init__(self):
        self.state = self.reset()

    def step(self, action):
        """
        Args:
            action: Action taken by agent

        Returns:
            next_state: New environment state
            reward: Reward for the action
            done: Whether episode is complete
        """
        next_state = self.compute_next_state(action)
        reward = self.compute_reward(action, next_state)
        done = self.is_terminal(next_state)

        return next_state, reward, done
```

**For LLMs:**

```python
# Policy = Language Model
policy = LanguageModel()

# Action = Next token choice
action = policy.sample_token(context)

# Environment = Task being solved
environment = ChessGame()  # or FlappyBird(), MathProblem(), etc.

# Training loop
for episode in range(num_episodes):
    state = environment.reset()

    while not done:
        # Model chooses action (token)
        action = policy.sample(state)

        # Environment responds
        next_state, reward, done = environment.step(action)

        # Update policy to maximize reward
        policy.update(state, action, reward)

        state = next_state
```

### **Example: Chess**

```python
class ChessEnvironment:
    def reset(self):
        self.board = Chess()
        return self.board.fen()  # Initial state

    def step(self, move):
        # Apply move
        self.board.push(move)

        # Check result
        if self.board.is_checkmate():
            reward = +1 if self.board.turn == BLACK else -1
            done = True
        elif self.board.is_stalemate():
            reward = 0
            done = True
        else:
            reward = 0  # No reward yet
            done = False

        return self.board.fen(), reward, done
```

### **Example: Math Problem Solving**

```python
class MathEnvironment:
    def __init__(self, problem, answer):
        self.problem = problem
        self.correct_answer = answer

    def reset(self):
        return f"Solve: {self.problem}\n\nWrite answer in \\boxed{{}}"

    def step(self, model_output):
        # Extract answer from model's response
        predicted = extract_boxed_answer(model_output)

        # Check correctness
        correct = (predicted == self.correct_answer)

        # Reward function
        reward = 0.0
        if has_boxed_format(model_output):
            reward += 0.1  # Bonus for proper formatting
        if correct:
            reward += 1.0  # Main reward for correctness

        return None, reward, True  # Episode ends after answer
```

### **RLHF: Learning from Human Preferences**

**The problem:** Many tasks have no clear "correct" answer

Examples:
- Is response A or B more helpful?
- Which summary is clearer?
- Which code is more readable?

**Solution:** Learn from comparisons

**RLHF workflow:**

**Stage 1: Collect preference data**

```python
# Show humans two responses
prompt = "Explain quantum computing to a 10-year-old"

response_a = model_v1.generate(prompt)
response_b = model_v2.generate(prompt)

# Human chooses which is better
human_feedback = {
    "prompt": prompt,
    "chosen": response_a,
    "rejected": response_b
}
```

**Stage 2: Train reward model**

```python
class RewardModel(nn.Module):
    def forward(self, prompt, response):
        # Returns scalar reward for response quality
        return self.model(prompt + response)

# Training
for batch in preference_data:
    # Predict rewards
    reward_chosen = reward_model(batch.prompt, batch.chosen)
    reward_rejected = reward_model(batch.prompt, batch.rejected)

    # Loss: chosen should have higher reward
    loss = -torch.log(torch.sigmoid(reward_chosen - reward_rejected))
    loss.backward()
```

**Stage 3: RL with learned rewards**

```python
# Use reward model as environment
for prompt in prompts:
    # Generate response
    response = policy.generate(prompt)

    # Score with reward model
    reward = reward_model(prompt, response)

    # Update policy to maximize reward
    policy.update(reward)
```

### **Real-World RLHF: ChatGPT Feedback**

When ChatGPT asks you:

> üëç üëé Which response was better?

That feedback:
1. Gets stored in preference database
2. Used to train reward models
3. Improves future versions via RLHF

**You are literally helping train the model!**

### **RLHF Challenges**

**1. Reward hacking**
```python
# Model learns to exploit reward model
# Example: Generate verbose, confident-sounding nonsense
# Reward model: "This sounds smart!" ‚Üí High reward
# Reality: Answer is wrong
```

**Solution:** Add KL divergence penalty to keep close to base model

**2. Reward model bias**
```python
# Reward model reflects annotator biases
# Example: Prefers certain writing styles
# May not match your users' preferences
```

**Solution:** Diverse annotator pool, careful evaluation

**3. Expensive data collection**
```python
# Human annotations are slow and costly
# Example: $0.10-1.00 per comparison
# Need 10K-1M comparisons ‚Üí $1K-1M budget
```

**Solution:** Active learning, synthetic preferences, DPO (see below)

---

## **9. System Prompts vs Fine-Tuning**

### **The System Prompt Explosion**

**Current common practice:**

```python
# Massive system prompt
system_prompt = """
You are ChessGPT, an expert chess engine.

RULES:
1. You must respond with moves in algebraic notation (e.g., "e4", "Nf3")
2. Never use long algebraic notation (e.g., "e2-e4")
3. Do not explain your moves unless asked
4. Do not use emojis or casual language
5. Always verify move legality
6. Consider tactical and positional factors
7. Prioritize king safety
8. Control the center
... (continues for 100+ lines)

MOVE FORMAT:
- Pawn moves: e4, d5
- Knight moves: Nf3, Nc6
- Bishop moves: Bc4, Bf5
... (continues for 50+ lines)

OPENING PRINCIPLES:
1. Control the center
2. Develop pieces
... (continues for 30+ lines)

Remember: ONLY output the move, nothing else!
"""

# Every single API call:
response = llm.generate(
    system=system_prompt,  # ~2000 tokens every time!
    user="Current position: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
)
```

**Cost analysis:**
```
Per request:
- System prompt: 2,000 tokens
- Input: 100 tokens
- Output: 5 tokens
- Total: 2,105 tokens

At $15 per 1M tokens:
- Cost per request: $0.032
- 1,000 requests/day: $32/day
- Annual: $11,680

System prompt alone: $10,950 annually!
```

### **The Fine-Tuning Alternative**

**Instead: Bake behavior into weights**

```python
# 1. Create training data that demonstrates desired behavior
training_data = [
    {
        "messages": [
            {"role": "system", "content": "You are a chess engine."},
            {"role": "user", "content": "Position: rnbqkbnr/... Your move?"},
            {"role": "assistant", "content": "e4"}
        ]
    },
    # ... 10,000 more examples
]

# 2. Fine-tune
chess_model = tinker.train(
    base_model="Llama-3.1-8B",
    data=training_data,
    task="chess"
)

# 3. Use with minimal prompt
response = chess_model.generate(
    system="You are a chess engine.",  # ~10 tokens
    user="Position: rnbqkbnr/... Your move?"
)
# Output: "e4"
```

**Cost analysis:**
```
Fine-tuning (one-time):
- Training: $200
- Time: 4 hours

Per request:
- System prompt: 10 tokens (99% reduction!)
- Input: 100 tokens
- Output: 5 tokens
- Total: 115 tokens

At $0.50 per 1M tokens (self-hosted):
- Cost per request: $0.00006
- 1,000 requests/day: $0.06/day
- Annual: $22

Total first year: $222
Savings: $11,458 (98% reduction!)
```

### **Comparison Table**

| Aspect | System Prompt | Fine-Tuning |
|--------|--------------|-------------|
| **Setup time** | Minutes | Hours-days |
| **Setup cost** | $0 | $100-1,000 |
| **Per-request cost** | High ($0.01-0.10) | Low ($0.0001-0.001) |
| **Token usage** | 1,000-10,000 extra | 10-100 extra |
| **Flexibility** | Easy to change | Requires retraining |
| **Consistency** | Can drift with updates | Stable |
| **Vendor dependency** | High | None (owned model) |
| **Break-even point** | N/A | ~1,000-10,000 requests |

### **When to Use Each Approach**

**Use system prompts when:**
- ‚úÖ Rapid prototyping
- ‚úÖ Requirements change frequently
- ‚úÖ Low request volume (<1,000/day)
- ‚úÖ Using frontier model capabilities
- ‚úÖ No ML expertise on team

**Use fine-tuning when:**
- ‚úÖ High request volume (>10,000/day)
- ‚úÖ Stable requirements
- ‚úÖ Cost is major concern
- ‚úÖ Need consistent behavior
- ‚úÖ Vendor independence important
- ‚úÖ Privacy/compliance requirements

### **Hybrid Approach**

**Best practice: Combine both**

```python
# 1. Fine-tune for core behavior
base_behavior_model = tinker.train(data=core_examples)

# 2. Use small prompt for task-specific variation
response = base_behavior_model.generate(
    system="Use formal tone for this query.",  # Task-specific
    user=query
)
```

---

## **10. Tinker Architecture: Abstracting GPUs**

### **The Traditional Fine-Tuning Pain**

**What you normally need to do:**

```bash
# 1. Provision GPUs
aws ec2 run-instances --instance-type p4d.24xlarge  # $32/hour

# 2. Install CUDA, drivers
sudo apt-get install nvidia-driver-535
sudo apt-get install cuda-toolkit-12-1

# 3. Set up distributed training
pip install torch torchrun
pip install deepspeed
pip install accelerate

# 4. Configure multi-GPU training
torchrun --nproc_per_node=8 train.py

# 5. Handle failures
# - NCCL errors
# - CUDA out of memory
# - Disk full during checkpointing
# - Network timeouts
# - Random crashes

# 6. Monitor and debug
nvidia-smi  # Check GPU usage
watch -n 1 'cat /proc/meminfo'  # Check memory
dmesg | grep -i error  # Check for hardware errors
```

**Time investment:**
- Initial setup: 1-3 days
- Per experiment: 2-8 hours
- Debugging failures: Hours to days

**Expertise required:**
- CUDA programming
- Distributed training (NCCL, FSDP)
- Linux system administration
- GPU architecture knowledge
- Network debugging

### **The Tinker Abstraction**

**What Tinker handles:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Tinker GPU Cluster              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚úÖ GPU allocation and management       ‚îÇ
‚îÇ  ‚úÖ CUDA and driver installation        ‚îÇ
‚îÇ  ‚úÖ Distributed training setup           ‚îÇ
‚îÇ  ‚úÖ Automatic failure recovery           ‚îÇ
‚îÇ  ‚úÖ Checkpoint storage (persistent)      ‚îÇ
‚îÇ  ‚úÖ Model parallelism and sharding       ‚îÇ
‚îÇ  ‚úÖ Gradient computation and updates     ‚îÇ
‚îÇ  ‚úÖ Network optimization                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üï API calls
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Your Local Machine (CPU)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚ö° Training loop logic                 ‚îÇ
‚îÇ  ‚ö° Data loading and preprocessing       ‚îÇ
‚îÇ  ‚ö° Reward computation (for RL)          ‚îÇ
‚îÇ  ‚ö° Metrics tracking and logging         ‚îÇ
‚îÇ  ‚ö° Experiment configuration             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Tinker Workflow**

**1. Installation (one-time):**

```bash
# Install client
pip install tinker

# Set API key
export TINKER_API_KEY="your-key-here"

# Clone cookbook (optional, for examples)
git clone https://github.com/tinker-ai/tinker-cookbook
cd tinker-cookbook
pip install -e .
```

**2. Basic training script:**

```python
from tinker import TrainingClient
from tinker.types import Datum

# Create client (handles GPU allocation behind the scenes)
client = TrainingClient(
    model_name="meta-llama/Llama-3.1-8B",
    lora_config={"rank": 32}
)

# Your data (prepared locally)
def prepare_data(examples):
    data = []
    for ex in examples:
        tokens = tokenizer.encode(ex["text"])
        data.append(Datum(
            tokens=tokens,
            targets=tokens[1:],  # Predict next token
            weights=[1.0] * len(tokens)
        ))
    return data

# Training loop (runs on your CPU)
for epoch in range(num_epochs):
    for batch in dataloader:
        # Prepare batch
        data = prepare_data(batch)

        # Forward/backward on Tinker's GPUs (async)
        future = client.forward_backward(data, loss_fn="cross_entropy")

        # Optimizer step on Tinker's GPUs (async)
        opt_future = client.optim_step()

        # Wait for results
        loss = future.result()
        opt_future.result()

        # Log (on your machine)
        print(f"Epoch {epoch}, Loss: {loss}")

    # Save checkpoint (stored in Tinker's cloud)
    checkpoint = client.save_weights_for_sampler()
    print(f"Saved: {checkpoint}")
```

**3. Sampling (inference):**

```python
# Sample from trained model
responses = client.sample(
    prompts=["What is the capital of France?"],
    params={
        "temperature": 0.7,
        "max_tokens": 100
    }
)

print(responses[0])  # "The capital of France is Paris."
```

### **Key Benefits**

**1. No GPU management:**
```python
# Traditional
gpus = allocate_gpus(count=8, type="A100")  # Your problem
setup_distributed(gpus)  # Your problem
train_model(gpus)  # Your problem
handle_failures(gpus)  # Your problem

# Tinker
client = TrainingClient(model_name="Llama-3.1-8B")  # Done!
```

**2. Async API for efficiency:**
```python
# Submit operations without waiting
fwd_bwd_future = client.forward_backward(data, loss_fn="cross_entropy")
optim_future = client.optim_step()

# Do other work on CPU while GPU is busy
metrics = compute_metrics(data)
log_to_wandb(metrics)

# Then wait for GPU operations
loss = fwd_bwd_future.result()
optim_future.result()
```

**3. Persistent checkpoints:**
```python
# Save
uri = client.save_state()
# Returns: "tinker://checkpoints/abc123..."

# Load later (even in different session)
client.load_state(uri)

# Continue training
for batch in more_data:
    client.forward_backward(batch)
    client.optim_step()
```

**4. Automatic scaling:**
```python
# Same code works for different model sizes
small_client = TrainingClient("Llama-3.1-8B")   # Uses 1-2 GPUs
large_client = TrainingClient("Llama-3.1-70B")  # Uses 8+ GPUs
huge_client = TrainingClient("Llama-3.1-405B")  # Uses 64+ GPUs

# Tinker handles distribution automatically
```

---

## **11. Prime Intellect and Distributed Training**

### **What is Prime Intellect?**

**Prime Intellect** is a peer-to-peer GPU network for distributed training:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Your GPU   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Network    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Other GPUs ‚îÇ
‚îÇ  (at home)  ‚îÇ     ‚îÇ  Coordinator‚îÇ     ‚îÇ  (worldwide)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚ñ≤                                        ‚ñ≤
      ‚îÇ                                        ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Training Job ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Concept:**
- Contribute your GPU when idle
- Get paid for compute time
- Use other people's GPUs for your training
- Decentralized, peer-to-peer network

### **How It Works**

**As a GPU provider:**
```python
# Install Prime Intellect client
pip install prime-intellect

# Register your GPU
prime setup --gpu-id 0

# Start providing compute
prime start
# Your GPU is now available for training jobs
# You earn credits/money while it runs
```

**As a user:**
```python
# Request compute
from prime import DistributedTrainer

trainer = DistributedTrainer(
    model="Llama-3.1-8B",
    num_gpus=8,  # Sourced from network
    budget_per_hour=5.0
)

# Train (automatically distributed across available GPUs)
trainer.train(dataset)
```

### **Benefits**

**For GPU providers:**
- üí∞ Monetize idle compute
- üìà Passive income from hardware
- üåç Support research community

**For users:**
- üíµ Lower costs than cloud providers
- üöÄ Access to diverse hardware
- üîì No vendor lock-in

### **Predefined Environments**

Prime Intellect provides training environments:

```python
from prime.envs import WordleEnv, WikiSearchEnv

# Wordle solver environment
wordle = WordleEnv()
state = wordle.reset()

while not done:
    guess = policy.predict(state)
    state, reward, done = wordle.step(guess)

# WikiSearch agent environment
wiki = WikiSearchEnv()
question = "Who invented the telephone?"

# Agent can search Wikipedia
search_results = wiki.search("telephone invention")
answer = policy.answer(question, search_results)
```

### **Integration with Tinker**

Tinker examples can work with Prime Intellect environments:

```python
from tinker import TrainingClient
from prime.envs import CustomEnv

# Define RL environment
env = CustomEnv()

# Train with Tinker
client = TrainingClient("Llama-3.1-8B")

for episode in range(num_episodes):
    state = env.reset()

    while not done:
        # Sample action from model
        action = client.sample([state])[0]

        # Environment step
        next_state, reward, done = env.step(action)

        # Update model
        client.forward_backward(
            data=create_rl_datum(state, action, reward),
            loss_fn="importance_sampling"
        )
        client.optim_step()
```

---

## **12. Evaluation and Competitive Moats**

### **The Evaluation Problem**

**Current state:** Most evaluation is "vibes-based"

```python
# Typical evaluation
model_a_output = "The capital of France is Paris."
model_b_output = "Paris is the capital of France."
# Evaluation: "Which is better?"
# ‚Üí Hard to automate, subjective
```

**Why this is a problem:**
- Can't measure improvement reliably
- Hard to catch regressions
- No clear optimization target
- Results don't generalize

### **Better Evaluation Approaches**

**1. Task-specific metrics:**

```python
# Code generation
def evaluate_code(generated_code, test_cases):
    metrics = {
        "syntax_valid": check_syntax(generated_code),
        "tests_passed": run_tests(generated_code, test_cases),
        "runtime": measure_runtime(generated_code),
        "memory": measure_memory(generated_code)
    }
    return metrics

# Math problems
def evaluate_math(model_output, correct_answer):
    predicted = extract_answer(model_output)
    return {
        "exact_match": predicted == correct_answer,
        "symbolic_equivalent": symbolic_equal(predicted, correct_answer),
        "has_correct_format": check_boxed_format(model_output),
        "reasoning_steps": count_reasoning_steps(model_output)
    }
```

**2. Automated benchmarks:**

| Domain | Benchmark | What It Measures |
|--------|-----------|------------------|
| General | MMLU | Multi-domain knowledge |
| Code | HumanEval | Python code generation |
| Math | GSM8K, MATH | Mathematical reasoning |
| Reasoning | BBH | Complex reasoning tasks |
| Truthfulness | TruthfulQA | Hallucination resistance |
| Safety | ToxiGen | Harmful content generation |

**3. User studies (expensive but valuable):**

```python
# A/B testing
def ab_test(model_a, model_b, users, queries):
    results = []

    for user, query in zip(users, queries):
        # Randomly assign model
        model = random.choice([model_a, model_b])
        response = model.generate(query)

        # Collect feedback
        rating = user.rate(response, scale=1-5)
        preference = user.preference  # A or B

        results.append({
            "model": model.name,
            "rating": rating,
            "preference": preference
        })

    # Statistical analysis
    return analyze_significance(results)
```

### **The Business Moat Question**

**Investors always ask:** "What's your competitive advantage?"

**For AI companies, potential moats:**

| Moat Type | Example | Strength | Durability |
|-----------|---------|----------|------------|
| **Data** | Cursor's code edits | Strong | Medium (can be replicated) |
| **Distribution** | Integration into VSCode | Strong | High (hard to displace) |
| **Network effects** | User feedback loop | Medium | High (compounds over time) |
| **Proprietary models** | Custom-trained models | Medium | Medium (models improve rapidly) |
| **Brand** | ChatGPT mindshare | Strong | High (hard to displace leader) |

### **Case Study: Cursor**

**Cursor's reported metrics:**
- ~$1B annual recurring revenue
- Deep integration in developer workflows
- Massive usage data collection

**Their moat strategy:**

```
User adoption
    ‚Üì
Collect usage data (code edits, completions, patterns)
    ‚Üì
Train specialized code models
    ‚Üì
Better product performance
    ‚Üì
More user adoption
    ‚Üì
(Virtuous cycle)
```

**Why they're raising billions:**
- To build their own models
- Reduce dependency on OpenAI/Anthropic
- Control their own destiny
- Capture more value

**Key insight:** The more users you have, the more data you collect, the better models you can train, the harder it is for competitors to catch up.

---

## **13. Vendor Risk and Model Dependency**

### **Real Vendor Risk Examples**

**Example 1: Windsurf IDE**

```
Before:
- Built on Anthropic's Claude models
- Excellent user experience
- Growing user base

After: (Anthropic restricts access or changes pricing)
- Product quality degrades
- Users churn
- Business model at risk
```

**Example 2: OpenAI model deprecation**

```
Timeline:
- 2023-01: Build product on GPT-4-turbo
- 2023-06: Optimize prompts, perfect workflow
- 2024-01: OpenAI announces GPT-4-turbo ‚Üí legacy
- 2024-06: Model deprecated, must migrate
- Result: Weeks of re-engineering, prompt rewrites
```

### **Dependency Risks**

**1. Pricing changes:**
```python
# Your product economics
cost_per_query = 0.01  # Using provider API
revenue_per_query = 0.015
profit_margin = 50%

# Provider increases prices 2x
new_cost_per_query = 0.02
new_profit = revenue_per_query - new_cost_per_query
# = -0.005 (LOSING MONEY!)

# Options:
# A. Increase prices (lose customers)
# B. Reduce quality (lose customers)
# C. Train own model (complex, expensive)
```

**2. Rate limiting:**
```python
# Black Friday: 100x traffic spike
normal_requests = 1000 / minute
spike_requests = 100000 / minute

# Provider rate limit: 10,000 / minute
# Your app: CRASHES for 90% of users
# Reputation: DAMAGED
```

**3. Model changes:**
```python
# You've optimized for GPT-4 behavior
optimized_prompt = """[10,000 tokens of carefully crafted prompt]"""

# OpenAI updates model (different behavior)
# Your prompts break, outputs degrade
# Must re-optimize from scratch
```

**4. Access revocation:**
```python
# Reasons providers might restrict access:
# - Your use case violates ToS
# - Capacity constraints
# - Strategic business reasons
# - Your company is now a competitor

# Result: Immediate business disruption
```

### **Mitigation Strategies**

**1. Multi-provider setup:**

```python
class MultiProviderLLM:
    def __init__(self):
        self.providers = [
            OpenAIProvider(),
            AnthropicProvider(),
            CohereProvider(),
            LocalModel()  # Fallback
        ]

    def generate(self, prompt):
        for provider in self.providers:
            try:
                return provider.generate(prompt)
            except (RateLimitError, AccessError) as e:
                logger.warning(f"{provider} failed: {e}")
                continue  # Try next provider

        raise AllProvidersFailed()
```

**2. Own your critical models:**

```python
# Critical path: Use owned model
critical_model = load_model("./models/our-8b-model")
response = critical_model.generate(query)

# Non-critical: Use provider API
enhancement = anthropic.generate(f"Enhance: {response}")
```

**3. Build with migration in mind:**

```python
# Abstraction layer
class LLMInterface:
    def generate(self, prompt: str) -> str:
        raise NotImplementedError

class OpenAILLM(LLMInterface):
    def generate(self, prompt: str) -> str:
        return openai.ChatCompletion.create(...)

class OurLLM(LLMInterface):
    def generate(self, prompt: str) -> str:
        return our_model.generate(...)

# Application code uses interface, not specific provider
def process_query(query: str, llm: LLMInterface):
    return llm.generate(query)

# Easy to swap providers
# llm = OpenAILLM()
llm = OurLLM()  # One line change!
```

**4. Synthetic data generation:**

```python
# Use provider API to generate training data
# Then train your own model

# Step 1: Generate synthetic data (one-time cost)
training_data = []
for example in seed_examples:
    # Use expensive frontier model
    synthetic = gpt5.generate(f"Generate 10 variations: {example}")
    training_data.extend(synthetic)

# Step 2: Train owned model (one-time cost)
our_model = tinker.train(
    base_model="Llama-3.1-8B",
    data=training_data
)

# Step 3: Use owned model (ongoing, cheap)
# No longer dependent on provider!
```

---

## **14. Tinker Cookbook and Example Recipes**

### **What is the Tinker Cookbook?**

The **Tinker Cookbook** is a collection of production-ready training recipes:

```
tinker-cookbook/
‚îú‚îÄ‚îÄ recipes/
‚îÇ   ‚îú‚îÄ‚îÄ chat_sl/              # Supervised chat fine-tuning
‚îÇ   ‚îú‚îÄ‚îÄ math_rl/              # Math reasoning with RL
‚îÇ   ‚îú‚îÄ‚îÄ preference/           # DPO and RLHF
‚îÇ   ‚îú‚îÄ‚îÄ tool_use/             # RAG and tool calling
‚îÇ   ‚îú‚îÄ‚îÄ prompt_distillation/  # Internalize prompts
‚îÇ   ‚îú‚îÄ‚îÄ multiplayer_rl/       # Multi-agent games
‚îÇ   ‚îî‚îÄ‚îÄ distillation/         # Teacher ‚Üí student
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ sl_loop.py           # Minimal SL example
‚îÇ   ‚îî‚îÄ‚îÄ rl_loop.py           # Minimal RL example
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ renderers/           # Chat format converters
    ‚îú‚îÄ‚îÄ datasets/            # Dataset loaders
    ‚îî‚îÄ‚îÄ evaluation/          # Eval frameworks
```

### **Recipe Categories**

**1. Chat SFT (Supervised Fine-Tuning)**

Train models for conversational AI:

```bash
python -m tinker_cookbook.recipes.chat_sl.train \
    model_name="meta-llama/Llama-3.1-8B" \
    dataset="HuggingFaceH4/no_robots" \
    batch_size=128 \
    learning_rate=1e-4
```

**Use cases:**
- Customer support chatbots
- Domain-specific assistants (legal, medical)
- Internal tools and documentation helpers

**2. Math RL (Reinforcement Learning)**

Train models to solve math problems:

```bash
python -m tinker_cookbook.recipes.math_rl.train \
    env="gsm8k" \
    model_name="meta-llama/Llama-3.1-8B-Instruct" \
    group_size=64 \
    batch_size=128
```

**Use cases:**
- Homework helpers
- Math tutoring systems
- Automated problem solving

**3. Preference Learning (DPO/RLHF)**

Train from human preferences:

```bash
python -m tinker_cookbook.recipes.preference.train \
    method="dpo" \
    dataset="Anthropic/hh-rlhf" \
    dpo_beta=0.1
```

**Use cases:**
- Aligning with user preferences
- Reducing harmful outputs
- Improving response quality

**4. Tool Use**

Train models to use external tools:

```bash
python -m tinker_cookbook.recipes.tool_use.train \
    tools="search,calculator,calendar" \
    dataset="custom_tool_dataset"
```

**Use cases:**
- RAG systems
- Multi-tool agents
- API-calling assistants

**5. Prompt Distillation**

Bake prompts into model weights:

```bash
python -m tinker_cookbook.recipes.prompt_distillation.train \
    teacher_model="gpt-4" \
    teacher_prompt="[Your long system prompt]" \
    student_model="Llama-3.1-8B"
```

**Use cases:**
- Reduce token costs
- Speed up inference
- Improve consistency

**6. Multi-Agent RL**

Train models for games and interactions:

```bash
python -m tinker_cookbook.recipes.multiplayer_rl.train \
    game="twenty_questions" \
    num_agents=2
```

**Use cases:**
- Game-playing AI
- Negotiation systems
- Debate and argumentation

**7. Distillation**

Transfer knowledge from large to small models:

```bash
python -m tinker_cookbook.recipes.distillation.train \
    teacher="Llama-3.1-70B" \
    student="Llama-3.1-8B" \
    method="on_policy"
```

**Use cases:**
- Deploy smaller, faster models
- Edge deployment
- Cost reduction

### **Cookbook Workflow**

**Typical workflow for adapting a recipe:**

1. **Choose closest recipe:**
   ```bash
   # Browse recipes
   ls tinker-cookbook/recipes/
   ```

2. **Review the recipe:**
   ```python
   # Read README.md to understand:
   # - What the recipe does
   # - Required dataset format
   # - Key hyperparameters
   # - Expected results
   ```

3. **Prepare your data:**
   ```python
   # Match the recipe's expected format
   # Example for chat_sl:
   data = [
       {
           "messages": [
               {"role": "system", "content": "You are a helpful assistant."},
               {"role": "user", "content": "What is Python?"},
               {"role": "assistant", "content": "Python is a programming language..."}
           ]
       },
       # ... more examples
   ]
   ```

4. **Configure training:**
   ```bash
   python -m tinker_cookbook.recipes.RECIPE.train \
       model_name="YOUR_MODEL" \
       learning_rate=LR \
       batch_size=BS \
       log_path="./logs/my-experiment" \
       wandb_project="my-project"
   ```

5. **Monitor and iterate:**
   ```python
   # Watch metrics
   tail -f ./logs/my-experiment/metrics.jsonl

   # Review samples
   open ./logs/my-experiment/transcripts.html

   # Adjust hyperparameters if needed
   ```

### **Example: Chess Bot**

**Goal:** Train a chess-playing model

**Approach:**

1. **Generate synthetic data:**
   ```python
   # Use GPT-4 to generate annotated games
   games = []
   for i in range(10000):
       prompt = f"Generate a chess game with annotations for position {i}"
       game = gpt4.generate(prompt)
       games.append(parse_pgn(game))
   ```

2. **Create dataset:**
   ```python
   training_data = []
   for game in games:
       for position, move in game:
           training_data.append({
               "messages": [
                   {"role": "system", "content": "You are a chess engine. Output only the move in algebraic notation."},
                   {"role": "user", "content": f"Position: {position.fen()}\nYour move?"},
                   {"role": "assistant", "content": move.uci()}
               ]
           })
   ```

3. **Train with Tinker:**
   ```bash
   python -m tinker_cookbook.recipes.chat_sl.train \
       model_name="meta-llama/Llama-3.1-8B" \
       dataset="./chess_games.jsonl" \
       batch_size=128 \
       learning_rate=5e-4 \
       max_steps=5000
   ```

4. **Evaluate:**
   ```python
   # Play against Stockfish
   wins = 0
   for game in test_games:
       result = play_game(our_model, stockfish_level_3)
       if result == "win":
           wins += 1

   print(f"Win rate: {wins / len(test_games)}")
   ```

---

## **15. Practical Next Steps and Final Takeaways**

### **Getting Started Checklist**

**Week 1: Learn the basics**
- [ ] Complete Tinker presentation (Lab 05 slides)
- [ ] Read `sl_loop.py` in Tinker Cookbook (~150 lines)
- [ ] Read `rl_loop.py` in Tinker Cookbook (~150 lines)
- [ ] Apply for Tinker API access
- [ ] Set up development environment

**Week 2: First experiments**
- [ ] Run pre-built recipe: `chat_sl` on NoRobots dataset
- [ ] Experiment with hyperparameters
- [ ] Evaluate results on validation set
- [ ] Save and load checkpoints

**Week 3: Custom application**
- [ ] Identify your use case
- [ ] Collect or generate training data
- [ ] Adapt closest recipe
- [ ] Train and evaluate your model

**Week 4: Production considerations**
- [ ] Benchmark inference speed and cost
- [ ] Set up monitoring and logging
- [ ] Plan deployment strategy
- [ ] Document your approach

### **Key Strategic Decisions**

**When to use frontier models:**
- ‚úÖ Prototyping and exploration
- ‚úÖ Generating synthetic training data
- ‚úÖ Complex reasoning tasks
- ‚úÖ Low-volume, high-value queries

**When to train your own models:**
- ‚úÖ High query volume (>10K/day)
- ‚úÖ Cost is major concern
- ‚úÖ Domain-specific needs
- ‚úÖ Privacy/compliance requirements
- ‚úÖ Vendor independence important

**When to use system prompts:**
- ‚úÖ Rapid iteration
- ‚úÖ Changing requirements
- ‚úÖ Using frontier capabilities
- ‚úÖ Low request volume

**When to fine-tune:**
- ‚úÖ Stable requirements
- ‚úÖ High volume
- ‚úÖ Cost optimization
- ‚úÖ Consistent behavior needed

### **Cost-Benefit Framework**

**Calculate your break-even:**

```python
def calculate_break_even(
    training_cost: float,
    api_cost_per_1m_tokens: float,
    owned_model_cost_per_1m_tokens: float,
    monthly_tokens: int
):
    """
    Calculate how long until fine-tuning pays off
    """
    monthly_api_cost = (monthly_tokens / 1_000_000) * api_cost_per_1m_tokens
    monthly_owned_cost = (monthly_tokens / 1_000_000) * owned_model_cost_per_1m_tokens
    monthly_savings = monthly_api_cost - monthly_owned_cost

    months_to_break_even = training_cost / monthly_savings

    return {
        "monthly_savings": monthly_savings,
        "break_even_months": months_to_break_even,
        "annual_savings": monthly_savings * 12 - training_cost
    }

# Example
result = calculate_break_even(
    training_cost=500,          # One-time
    api_cost_per_1m_tokens=15,  # Claude Sonnet
    owned_model_cost_per_1m_tokens=0.50,  # Self-hosted 8B
    monthly_tokens=10_000_000   # 10M tokens/month
)

print(f"Monthly savings: ${result['monthly_savings']:.2f}")
print(f"Break even in: {result['break_even_months']:.1f} months")
print(f"First year savings: ${result['annual_savings']:.2f}")

# Output:
# Monthly savings: $145.00
# Break even in: 3.4 months
# First year savings: $1,240.00
```

### **Common Pitfalls and How to Avoid Them**

**1. Overfitting on small datasets**
```python
# Problem: 1,000 examples, train for 10,000 steps
# Model memorizes training data, performs poorly on new data

# Solution: Early stopping
best_val_loss = float('inf')
patience = 5
no_improvement = 0

for step in range(max_steps):
    train_loss = train_step()

    if step % 100 == 0:
        val_loss = evaluate(val_set)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_checkpoint()
            no_improvement = 0
        else:
            no_improvement += 1

        if no_improvement >= patience:
            print("Early stopping!")
            break
```

**2. Wrong learning rate for LoRA**
```python
# Problem: Using base model LR for LoRA
learning_rate = 5e-5  # Too low for LoRA!

# Solution: Use provided utilities
from tinker_cookbook.utils import get_lr
learning_rate = get_lr("meta-llama/Llama-3.1-8B")  # Correct!
```

**3. Poor data quality**
```python
# Problem: Noisy, inconsistent data
data = scrape_web()  # Lots of junk

# Solution: Filter and clean
def filter_quality(examples):
    filtered = []
    for ex in examples:
        if (
            len(ex["text"]) > 100 and  # Not too short
            len(ex["text"]) < 10000 and  # Not too long
            is_coherent(ex["text"]) and  # Makes sense
            not contains_spam(ex["text"])  # No spam
        ):
            filtered.append(ex)
    return filtered
```

**4. Not evaluating properly**
```python
# Problem: Only looking at training loss
# ‚Üí Can't tell if model is actually improving

# Solution: Comprehensive evaluation
def evaluate_model(model, test_set):
    metrics = {
        "loss": compute_loss(model, test_set),
        "accuracy": compute_accuracy(model, test_set),
        "perplexity": compute_perplexity(model, test_set),

        # Task-specific
        "task_success_rate": evaluate_task(model, test_set),

        # Qualitative
        "sample_outputs": generate_samples(model, test_prompts)
    }
    return metrics
```

### **Final Key Takeaways**

**1. The landscape is changing rapidly**
- Tools like Tinker make fine-tuning accessible
- Open-weight models are catching up to frontier models
- Cost curves favor owning your own models

**2. Data is key**
- Quality > quantity
- Synthetic data is viable
- Your unique data is your moat

**3. Start simple, iterate**
- Begin with cookbook recipes
- Small models first (8B)
- Scale when you've validated approach

**4. Measure everything**
- Track costs (training and inference)
- Monitor quality metrics
- Compare to baselines

**5. Plan for the future**
- Reduce vendor dependency
- Build with migration in mind
- Invest in evaluation infrastructure

### **Resources and Further Reading**

**Official Documentation:**
- Tinker API: https://docs.tinker.ai
- Tinker Cookbook: https://github.com/tinker-ai/tinker-cookbook
- LoRA paper: https://arxiv.org/abs/2106.09685

**Datasets:**
- NoRobots: https://huggingface.co/datasets/HuggingFaceH4/no_robots
- GSM8K: https://github.com/openai/grade-school-math
- Anthropic HHH: https://github.com/anthropics/hh-rlhf

**Tools:**
- Hugging Face: https://huggingface.co
- Weights & Biases: https://wandb.ai
- Prime Intellect: https://primeintellect.ai

**Community:**
- Tinker Discord: [Link from Tinker docs]
- r/LocalLLaMA: https://reddit.com/r/LocalLLaMA
- EleutherAI Discord: https://discord.gg/eleutherai

---

## **16. Assignment Ideas**

**Beginner Level:**

1. **Fine-tune a chat model on custom domain**
   - Use `chat_sl` recipe
   - Create 100-500 examples for your domain
   - Evaluate against base model

2. **Prompt distillation experiment**
   - Take a complex system prompt
   - Generate synthetic data with frontier model
   - Train small model to internalize behavior
   - Compare costs and quality

**Intermediate Level:**

3. **Math reasoning with RL**
   - Use `math_rl` recipe on GSM8K
   - Implement custom reward function
   - Measure improvement over SFT baseline

4. **Tool-using agent**
   - Define structured data schema
   - Train model to query database
   - Evaluate tool-use accuracy

**Advanced Level:**

5. **Multi-model system**
   - Small model for common queries (fast, cheap)
   - Large model for complex queries (slow, expensive)
   - Router to decide which model to use

6. **Custom RL environment**
   - Define your own task (game, problem-solving)
   - Implement reward function
   - Train and evaluate

---

### **IMPORTANT**

> **The opportunity is now:** Tools like Tinker abstract away the complexity of distributed training, making it feasible for small teams and individuals to train custom models. The companies that win will be those that:
>
> 1. **Own their models** (reduce vendor risk)
> 2. **Have proprietary data** (unique training sets)
> 3. **Iterate quickly** (experiment-driven development)
> 4. **Measure rigorously** (data-driven decisions)
>
> The ecosystem is exponentially more complex than a year ago, but also exponentially more capable. Start experimenting today.

---

**Acknowledgments:**
- Thinking Machines / Tinker team for API access
- Tinker Cookbook contributors
- Open-source model communities (Meta, Mistral, etc.)

**Course Information:**
- **Course:** Scientific Programming (PMID16)
- **Institution:** Faculty of Science, University of Split
- **Instructor:** Nikola Baliƒá
- **Lab:** 05 - Training Custom Language Models

