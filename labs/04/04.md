# **Scientific Programming – Lab 4**

## Web Scraping and AI-Powered Content Analysis

### **Session Overview**

This lab introduces students to modern web scraping workflows combined with AI-powered content analysis. Students learn to:

* Use browser automation APIs (Steel) for reliable web scraping
* Integrate large language models (Google Gemini) for content analysis
* Build interactive data collection tools with Marimo notebooks
* Handle API authentication and environment configuration
* Process and parse HTML content programmatically
* Apply AI to extract structured information from unstructured web data

**Real-world applications:**

* News monitoring and summarization
* Market research and competitor analysis
* Data collection for research projects
* Content aggregation and curation
* Price monitoring and comparison

---

## **Learning Objectives**

By the end of this lab, you should be able to:

1. Understand when and why to use browser automation services vs. direct HTTP requests
2. Configure and authenticate with third-party APIs (Steel, Google Gemini)
3. Build interactive scraping applications using Marimo notebooks
4. Handle HTML parsing and text extraction with BeautifulSoup4
5. Securely manage API keys using environment variables and .env files
6. Implement error handling for API rate limits and authentication failures
7. Design effective prompts for AI content analysis
8. Process and structure data extracted from websites
9. Navigate common web scraping challenges (dynamic content, authentication, rate limits)

---

## **Prerequisites**

* Completion of Lab 01 (Python environment setup, Marimo basics, Google Gemini API)
* Basic understanding of HTML structure
* Familiarity with API concepts
* Python environment with `uv` or `pip` installed

---

## **1. Introduction to Web Scraping**

### **Why Web Scraping?**

Web scraping enables automated data collection from websites when:

* No official API is available
* Manual data collection is impractical
* You need to monitor changes over time
* You're conducting research requiring web data

### **Scraping Approaches**

| Approach                           | When to Use                  | Pros                                          | Cons                                   |
| ---------------------------------- | ---------------------------- | --------------------------------------------- | -------------------------------------- |
| **Direct HTTP (requests)**         | Simple, static websites      | Fast, free, no dependencies                   | Doesn't handle JavaScript, easily blocked |
| **Browser Automation (Steel)**     | Dynamic content, complex sites | Handles JavaScript, realistic browser behavior | Costs money, slower, requires API      |
| **HTML Parsing (BeautifulSoup)**   | Processing scraped HTML      | Flexible, powerful parsing                    | Requires valid HTML structure          |

### **Legal and Ethical Considerations**

⚠️ **Important:**

* Always check website's `robots.txt` and Terms of Service
* Respect rate limits and implement delays
* Don't overload servers with requests
* Only scrape publicly available data
* Consider data privacy regulations (GDPR, etc.)
* Provide proper attribution when using scraped data

---

## **2. Setting Up Steel API**

Steel (https://steel.dev) provides browser automation as a service, handling:

* JavaScript rendering
* Session management
* Proxy rotation
* CAPTCHA handling
* Realistic browser fingerprints

### **Getting Your Steel API Key**

1. Visit https://steel.dev and sign up
2. Navigate to your dashboard
3. Create a new API key
4. Copy the key (starts with `ste-`)

### **Steel API Basics**

**Endpoint:** `https://api.steel.dev/v1/scrape`

**Basic Request Structure:**

```python
import requests

response = requests.post(
    "https://api.steel.dev/v1/scrape",
    headers={
        "steel-api-key": "your_key_here",
        "content-type": "application/json",
        "accept": "application/json",
    },
    json={
        "url": "https://example.com",
        "extract": "text",  # or "html"
        "useProxy": False,
        "delay": 1,  # seconds to wait for page load
        "fullPage": True,
        "region": "",
    }
)

data = response.json()
```

### **Understanding Steel Response Formats**

Steel can return data in multiple structures. Here's robust code to handle them:

```python
from bs4 import BeautifulSoup
import json

def extract_text_from_steel_response(response_data):
    """
    Handles multiple Steel API response formats
    """
    if isinstance(response_data, dict):
        # Try direct text fields first
        for key in ("text", "content", "extracted_text"):
            val = response_data.get(key)
            if isinstance(val, str) and val.strip():
                return val

        # Check nested content.html structure
        content = response_data.get("content")
        if isinstance(content, dict):
            html = content.get("html") or content.get("body")
            if isinstance(html, str) and html.strip():
                soup = BeautifulSoup(html, "html.parser")
                return soup.get_text(" ", strip=True)

        # Check top-level html field
        html = response_data.get("html")
        if isinstance(html, str) and html.strip():
            soup = BeautifulSoup(html, "html.parser")
            return soup.get_text(" ", strip=True)

        # Fallback: stringify JSON
        return json.dumps(response_data, ensure_ascii=False)

    if isinstance(response_data, list):
        return "\n".join(str(item) for item in response_data)

    return str(response_data)
```

---

## **3. Working with Google Gemini API**

### **Review: Getting Your Gemini API Key**

(Reference Lab 01 for detailed instructions)

1. Visit https://aistudio.google.com/app/apikey
2. Sign in with your Google account
3. Click "Create API Key"
4. Copy the key (starts with `AIzaSy`)

### **Model Selection**

Different Gemini models offer various trade-offs:

| Model              | Speed    | Quality      | Cost | Best For              |
| ------------------ | -------- | ------------ | ---- | --------------------- |
| `gemini-2.0-flash` | ⚡⚡⚡ | ⭐⭐⭐    | $    | Most use cases        |
| `gemini-1.5-flash` | ⚡⚡   | ⭐⭐⭐    | $    | Stable, reliable      |
| `gemini-1.5-pro`   | ⚡       | ⭐⭐⭐⭐⭐ | $$$  | Complex analysis      |
| `gemini-2.5-flash` | ⚡⚡⚡ | ⭐⭐⭐⭐  | $$   | Balanced option       |

### **Dynamic Model Selection with Fallback**

Handle quota errors gracefully by trying multiple models:

```python
import google.generativeai as genai

def generate_with_fallback(prompt: str, api_key: str) -> str:
    """
    Tries multiple Gemini models until one succeeds
    """
    genai.configure(api_key=api_key)

    # Get available models dynamically
    try:
        models = list(genai.list_models())
        candidate_models = []
        for m in models:
            methods = getattr(m, "supported_generation_methods", []) or []
            if "generateContent" in methods:
                candidate_models.append(getattr(m, "name", ""))
    except Exception:
        candidate_models = []

    # Add fallback models
    candidate_models += [
        "gemini-2.0-flash",
        "gemini-1.5-flash",
        "gemini-1.5-pro",
    ]

    last_error = None
    for model_name in candidate_models:
        try:
            if not model_name:
                continue
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return getattr(response, "text", "") or "(empty response)"
        except Exception as e:
            last_error = e
            continue

    return f"ERROR: All models failed. Last error: {last_error}"
```

### **Effective Prompt Engineering**

**Generic summarization (not ideal):**

```python
prompt = f"Summarize this text: {text}"
```

**Specific, structured prompt (better):**

```python
prompt = f"""
Analyze the following web content from a news website.

Extract and provide:
1. Main headline/topic
2. Top 5 most important stories
3. For each story: title, brief summary (1 sentence), key details
4. Overall themes and trends

Format your response as a clear, bulleted list.
Avoid JSON or code blocks - use plain text with bullet points.

Content:
{text[:8000]}
"""
```

**Task-specific prompt (best):**

```python
# For news analysis
prompt = f"""
You are a news editor analyzing today's headlines.
Extract the top 5 stories with:
- Headline
- Category (politics/economy/tech/etc)
- Impact level (high/medium/low)
- 1-sentence summary

Content: {text}
"""

# For product comparison
prompt = f"""
You are analyzing product listings.
For each item, extract:
- Product name
- Price
- Condition (new/used)
- Key features
- Deal quality (excellent/good/fair)

Listings: {text}
"""
```

---

## **4. Environment Configuration and Security**

### **Managing API Keys Securely**

**❌ Never do this:**

```python
STEEL_API_KEY = "ste-abc123xyz"  # Hardcoded!
GOOGLE_API_KEY = "AIzaSy..."     # Will be in git!
```

**✅ Always use environment variables:**

**Step 1: Create `.env` file**

```bash
# .env
STEEL_API_KEY=ste-your-key-here
GOOGLE_API_KEY=AIzaSy-your-key-here
GEMINI_API_KEY=AIzaSy-your-key-here  # Some use this name
```

**Step 2: Add `.env` to `.gitignore`**

```bash
# .gitignore
.env
*.env
```

**Step 3: Load in Python**

```python
import os
from dotenv import load_dotenv
from pathlib import Path

# Load from multiple possible locations
load_dotenv()  # Load from current directory

# Also try script's directory (important in Marimo!)
try:
    load_dotenv(dotenv_path=Path(__file__).with_name(".env"), override=False)
except Exception:
    pass

# Get keys with fallbacks
STEEL_API_KEY = os.getenv("STEEL_API_KEY", "")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY", "")
```

### **Common .env Loading Issues**

**Problem:** "Google API key not loading from .env"

**Solutions:**

1. Check for typos: `GOOGLE_API_KEY` vs `GEMINI_API_KEY`
2. Ensure no spaces around `=`: `KEY=value` not `KEY = value`
3. No quotes needed: `KEY=abc123` not `KEY="abc123"`
4. Check file location relative to script
5. Verify file is named exactly `.env` not `env` or `.env.txt`

**Debugging helper:**

```python
print(f"Steel key loaded: {bool(STEEL_API_KEY)}")
print(f"Google key loaded: {bool(GOOGLE_API_KEY)}")
print(f"Current working directory: {os.getcwd()}")
print(f"Script location: {Path(__file__).parent}")
```

---

## **5. Building Interactive Scrapers with Marimo**

### **Basic Marimo Structure for Scraping**

**Cell 1: Imports**

```python
import marimo as mo
app = mo.App()

@app.cell
def _():
    import os
    import requests
    from dotenv import load_dotenv
    import google.generativeai as genai
    from bs4 import BeautifulSoup
    return os, requests, load_dotenv, genai, BeautifulSoup
```

**Cell 2: Load Configuration**

```python
@app.cell
def _(load_dotenv, os):
    from pathlib import Path
    load_dotenv()
    try:
        load_dotenv(dotenv_path=Path(__file__).with_name(".env"), override=False)
    except Exception:
        pass

    STEEL_API_KEY = os.getenv("STEEL_API_KEY", "")
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")

    return STEEL_API_KEY, GOOGLE_API_KEY
```

**Cell 3: UI Components**

```python
@app.cell
def _():
    import marimo as mo

    url_input = mo.ui.text(
        label="URL to Scrape",
        placeholder="https://example.com",
        value=""
    )

    run_button = mo.ui.run_button(label="Scrape & Analyze")

    mo.vstack([
        mo.md("## Web Scraper"),
        url_input,
        run_button
    ])
    return url_input, run_button
```

**Cell 4: Core Logic**

```python
@app.cell
def _(run_button, url_input, STEEL_API_KEY, GOOGLE_API_KEY, requests, genai):
    import marimo as mo

    def scrape_with_steel(url, api_key):
        # Steel scraping logic here
        pass

    def analyze_with_gemini(text, api_key):
        # Gemini analysis logic here
        pass

    result = mo.md("Click 'Scrape & Analyze' to start")

    if run_button.value:
        url = url_input.value.strip()

        if not url:
            result = mo.md("❌ Please enter a URL")
        elif not STEEL_API_KEY or not GOOGLE_API_KEY:
            result = mo.md("❌ API keys not configured")
        else:
            result = mo.md("⏳ Scraping...")
            scraped_text = scrape_with_steel(url, STEEL_API_KEY)

            if scraped_text.startswith("ERROR"):
                result = mo.md(f"❌ {scraped_text}")
            else:
                result = mo.md("⏳ Analyzing with AI...")
                analysis = analyze_with_gemini(scraped_text, GOOGLE_API_KEY)

                result = mo.md(f"""
                ### Results

                **URL:** {url}

                **Scraped Content (first 500 chars):**
                ```
                {scraped_text[:500]}
                ```

                **AI Analysis:**
                {analysis}
                """)

    result
```

### **Advanced UI Patterns**

**Password-style input for API keys:**

```python
steel_input = mo.ui.text(
    label="Steel API Key",
    value=STEEL_API_KEY,
    kind="password"
)
```

**Multiple input fields with layout:**

```python
mo.vstack([
    mo.md("### Configuration"),
    mo.hstack([steel_input, google_input]),
    mo.md("---"),
    mo.md("### Scraper"),
    url_input,
    run_button
])
```

**Download results:**

```python
download_btn = mo.download(
    data=results_text.encode('utf-8'),
    filename="scraping_results.md",
    mimetype="text/markdown",
    label="⬇️ Download Results"
)
```

---

## **6. Complete Working Example**

Here's a complete, production-ready scraper combining all concepts:

```python
import marimo as mo

app = mo.App()

@app.cell
def _():
    import os
    import requests
    from dotenv import load_dotenv
    import google.generativeai as genai
    from bs4 import BeautifulSoup
    import json
    return os, requests, load_dotenv, genai, BeautifulSoup, json

@app.cell
def _(load_dotenv, os):
    from pathlib import Path
    load_dotenv()
    try:
        load_dotenv(dotenv_path=Path(__file__).with_name(".env"), override=False)
    except Exception:
        pass

    STEEL_API_KEY = os.getenv("STEEL_API_KEY", "")
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")

    return STEEL_API_KEY, GOOGLE_API_KEY

@app.cell
def _():
    import marimo as mo

    mo.md("""
    # Web Scraper & AI Analyzer

    This tool scrapes any website and analyzes the content using AI.
    """)
    return

@app.cell
def _(STEEL_API_KEY, GOOGLE_API_KEY):
    import marimo as mo

    steel_input = mo.ui.text(
        label="Steel API Key",
        value=STEEL_API_KEY,
        kind="password"
    )

    google_input = mo.ui.text(
        label="Google AI API Key",
        value=GOOGLE_API_KEY,
        kind="password"
    )

    mo.vstack([
        mo.md("### API Configuration"),
        steel_input,
        google_input
    ])
    return steel_input, google_input

@app.cell
def _():
    import marimo as mo

    url_input = mo.ui.text(
        label="Website URL",
        placeholder="https://news.ycombinator.com/",
        value=""
    )

    run_btn = mo.ui.run_button(label="Scrape & Analyze")

    mo.vstack([
        mo.md("---"),
        mo.md("### Website to Scrape"),
        url_input,
        run_btn
    ])
    return url_input, run_btn

@app.cell
def _(requests, BeautifulSoup, json, genai, run_btn, url_input, steel_input, google_input):
    import marimo as mo

    def scrape_with_steel(url: str, api_key: str) -> str:
        """Scrapes a URL using Steel API"""
        if not api_key:
            return "ERROR: Steel API key not provided"

        try:
            resp = requests.post(
                "https://api.steel.dev/v1/scrape",
                headers={
                    "steel-api-key": api_key,
                    "content-type": "application/json",
                    "accept": "application/json",
                },
                json={
                    "url": url,
                    "extract": "text",
                    "useProxy": False,
                    "delay": 1,
                    "fullPage": True,
                },
                timeout=30
            )

            if resp.status_code != 200:
                return f"ERROR: Steel API returned status {resp.status_code}"

            data = resp.json()

            # Handle multiple response formats
            if isinstance(data, dict):
                for key in ("text", "content", "extracted_text"):
                    val = data.get(key)
                    if isinstance(val, str) and val.strip():
                        return val

                content = data.get("content")
                if isinstance(content, dict):
                    html = content.get("html") or content.get("body")
                    if isinstance(html, str) and html.strip():
                        soup = BeautifulSoup(html, "html.parser")
                        return soup.get_text(" ", strip=True)

                html = data.get("html")
                if isinstance(html, str) and html.strip():
                    soup = BeautifulSoup(html, "html.parser")
                    return soup.get_text(" ", strip=True)

                return json.dumps(data, ensure_ascii=False)

            return str(data)

        except Exception as e:
            return f"ERROR: {type(e).__name__}: {e}"

    def analyze_with_gemini(text: str, api_key: str) -> str:
        """Analyzes text using Google Gemini"""
        if not api_key:
            return "ERROR: Google API key not provided"

        try:
            genai.configure(api_key=api_key)

            # Try multiple models
            models_to_try = [
                "gemini-2.0-flash",
                "gemini-1.5-flash",
                "gemini-1.5-pro"
            ]

            prompt = f"""
            Analyze the following web content and provide:

            1. Main topic/subject (1 sentence)
            2. Key points or highlights (3-5 bullet points)
            3. Overall summary (2-3 sentences)

            Format your response clearly with headers and bullet points.

            Content (first 8000 characters):
            {text[:8000]}
            """

            for model_name in models_to_try:
                try:
                    model = genai.GenerativeModel(model_name)
                    response = model.generate_content(prompt)
                    return response.text
                except Exception:
                    continue

            return "ERROR: All Gemini models failed (quota or rate limit)"

        except Exception as e:
            return f"ERROR: {type(e).__name__}: {e}"

    view = mo.md("Enter a URL and click 'Scrape & Analyze' to begin")

    if run_btn.value:
        steel_key = steel_input.value.strip()
        google_key = google_input.value.strip()
        url = url_input.value.strip()

        if not url:
            view = mo.md("❌ Please enter a URL")
        elif not steel_key:
            view = mo.md("❌ Steel API key required")
        elif not google_key:
            view = mo.md("❌ Google API key required")
        else:
            view = mo.md(f"⏳ Scraping {url}...")

            scraped_text = scrape_with_steel(url, steel_key)

            if scraped_text.startswith("ERROR"):
                view = mo.md(f"❌ Scraping failed:\n\n{scraped_text}")
            else:
                view = mo.md("⏳ Analyzing content with AI...")

                analysis = analyze_with_gemini(scraped_text, google_key)

                excerpt = scraped_text[:500].replace("\n", " ")

                results = f"""
                # Scraping Results

                **URL:** {url}

                **Content Length:** {len(scraped_text)} characters

                ---

                ## Content Preview (first 500 chars)

                ```
                {excerpt}...
                ```

                ---

                ## AI Analysis

                {analysis}

                ---

                *Scraped and analyzed successfully*
                """

                download_btn = mo.download(
                    data=results.encode('utf-8'),
                    filename=f"scraping_results_{url.split('//')[-1].split('/')[0]}.md",
                    mimetype="text/markdown",
                    label="⬇️ Download Results"
                )

                view = mo.vstack([
                    mo.md(results),
                    download_btn
                ])

    view

if __name__ == "__main__":
    app.run()
```

---

## **7. Common Use Cases and Examples**

### **Use Case 1: News Aggregation**

**Target:** Hacker News (https://news.ycombinator.com/)

**Goal:** Extract top 10 stories with titles, scores, and summaries

**Prompt:**

```python
prompt = f"""
Analyze this Hacker News homepage content.

Extract the TOP 10 stories and for each provide:
- Title
- Score (points)
- Number of comments
- Brief description (1 sentence about what it's about)

At the end, provide a 2-sentence summary of today's main themes on Hacker News.

Format as a numbered list.

Content: {text}
"""
```

### **Use Case 2: Weather Monitoring**

**Target:** Meteo.hr (https://meteo.hr)

**Goal:** Extract weather forecast in friendly format

**Prompt:**

```python
prompt = f"""
You are a friendly radio weather presenter.

Transform this official weather forecast into a casual, easy-to-understand format.

Include:
- Today's weather conditions
- Temperature range
- Wind conditions
- Precipitation chances
- Any weather warnings

Write conversationally but informatively.

Forecast data: {text}
"""
```

### **Use Case 3: E-commerce Price Monitoring**

**Target:** Product listing page (e.g., PlayStation Store)

**Goal:** Extract product details and prices

**Prompt:**

```python
prompt = f"""
Extract product information from this e-commerce page.

For each product, list:
- Product name
- Price (with currency)
- Availability (in stock / out of stock)
- Key features or specs
- Special offers or discounts

Format as a clean table or bulleted list.

Page content: {text}
"""
```

### **Use Case 4: Research Paper Monitoring**

**Target:** ArXiv or research site

**Goal:** Summarize recent papers

**Prompt:**

```python
prompt = f"""
You are a research assistant reviewing academic papers.

Summarize the papers found on this page:
- Title
- Authors
- Key research question
- Main findings (1-2 sentences)
- Significance

Focus on papers related to [your research area].

Content: {text}
"""
```

---

## **8. Troubleshooting Guide**

| Issue                          | Symptom                         | Solution                                                                                                                                                                                                      |
| ------------------------------ | ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **API Key Not Loading**        | "API key not set" error         | 1. Check `.env` file exists<br>2. Verify file name (no spaces)<br>3. Check key names match<br>4. Print `os.getcwd()` to verify location<br>5. Use absolute path to `.env`                                    |
| **Steel 401 Error**            | "Unauthorized"                  | 1. Verify API key is correct<br>2. Check header format: `steel-api-key`<br>3. Ensure key has proper permissions                                                                                              |
| **Steel 404 Error**            | "Not found"                     | 1. Check endpoint URL<br>2. Verify API version (`/v1/scrape`)<br>3. Check Steel service status                                                                                                               |
| **Empty Steel Response**       | No text extracted               | 1. Try `extract: "html"` instead of `"text"`<br>2. Use BeautifulSoup to parse<br>3. Increase `delay` parameter<br>4. Check if site requires authentication                                                   |
| **Gemini Quota Error**         | "Resource exhausted" or 429     | 1. Implement model fallback<br>2. Wait 60 seconds and retry<br>3. Use smaller model (flash)<br>4. Reduce prompt length<br>5. Check API quota limits                                                          |
| **Gemini Empty Response**      | `(empty response)`              | 1. Check prompt clarity<br>2. Verify model name is correct<br>3. Try different model<br>4. Check content isn't filtered (safety)                                                                             |
| **HTML Instead of Text**       | Raw HTML tags in output         | 1. Use BeautifulSoup: `soup.get_text()`<br>2. Set Steel `extract: "text"`<br>3. Post-process with regex                                                                                                      |
| **Rate Limiting**              | Too many requests error         | 1. Add delays between requests<br>2. Implement exponential backoff<br>3. Reduce scraping frequency                                                                                                           |
| **Marimo Not Finding .env**    | Keys work in terminal but not Marimo | 1. Use `Path(__file__).with_name(".env")`<br>2. Print `__file__` to see script location<br>3. Use absolute path<br>4. Add manual input fields as fallback                                                    |
| **Timeout Errors**             | Connection timeout              | 1. Increase `timeout` parameter<br>2. Check internet connection<br>3. Try different time of day<br>4. Check if site is blocking requests                                                                     |

---

## **9. Best Practices**

### **Code Organization**

✅ **Do:**

* Separate scraping, parsing, and analysis functions
* Use type hints for function parameters
* Add docstrings explaining what each function does
* Handle errors gracefully with try/except
* Log important steps for debugging

❌ **Don't:**

* Put all code in one giant function
* Hardcode URLs or API keys
* Ignore error handling
* Skip input validation

### **Performance**

✅ **Do:**

* Cache scraped data when possible
* Set reasonable timeouts (30-60 seconds)
* Use `delay` parameter in Steel for page rendering
* Limit text sent to Gemini (first 8000 chars often enough)

❌ **Don't:**

* Scrape the same page repeatedly
* Send entire HTML documents to Gemini
* Make parallel requests without rate limiting
* Ignore timeout parameters

### **Prompt Design**

✅ **Do:**

* Be specific about output format
* Provide examples when needed
* Specify language (Croatian/English)
* Break complex tasks into steps
* Include context about the data source

❌ **Don't:**

* Use vague prompts like "analyze this"
* Ask for multiple unrelated things at once
* Assume the AI knows the context
* Forget to specify output language

### **Error Handling**

✅ **Do:**

```python
try:
    result = risky_operation()
except SpecificError as e:
    logger.error(f"Specific error: {e}")
    return fallback_value
except Exception as e:
    logger.error(f"Unexpected error: {e}")
    return error_message
```

❌ **Don't:**

```python
try:
    result = risky_operation()
except:
    pass  # Silently fail
```

---

## **10. Exercise Ideas**

### **Beginner:**

1. Scrape a simple news site and count articles
2. Extract weather data and display temperature
3. Get top stories from Hacker News
4. Monitor a blog for new posts

### **Intermediate:**

5. Compare prices across multiple e-commerce sites
6. Track changes on a specific webpage over time
7. Build a news aggregator from multiple sources
8. Create a research paper monitoring tool

### **Advanced:**

9. Build a competitive analysis dashboard
10. Create an automated report generator
11. Implement RSS-style feed from any website
12. Build a personalized news digest system

---

## **11. Additional Resources**

**Documentation:**

* Steel API: https://docs.steel.dev
* Google Gemini: https://ai.google.dev/docs
* Marimo: https://docs.marimo.io
* BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
* python-dotenv: https://pypi.org/project/python-dotenv/

**Useful Libraries:**

```bash
uv add marimo requests beautifulsoup4 google-generativeai python-dotenv lxml
```

Or with pip:

```bash
pip install marimo requests beautifulsoup4 google-generativeai python-dotenv lxml
```

**Web Scraping Ethics:**

* https://www.eff.org/issues/coders/reverse-engineering-faq
* https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/

---

## **12. Project Ideas for Final Assignment**

Students could build:

1. **News Monitor** - Track specific topics across multiple news sites
2. **Price Tracker** - Monitor product prices and alert on deals
3. **Research Assistant** - Aggregate papers from multiple sources
4. **Weather Dashboard** - Multi-location weather monitoring
5. **Social Media Aggregator** - Collect public posts on a topic
6. **Job Board Scraper** - Aggregate job postings
7. **Event Calendar** - Scrape events from multiple sources
8. **Real Estate Monitor** - Track property listings
9. **Sports Tracker** - Collect game scores and statistics
10. **Academic Deadline Tracker** - Monitor conference deadlines

---

## **Reflection and Next Steps**

This lab teaches a complete workflow for:

1. Identifying data sources on the web
2. Scraping data reliably with browser automation
3. Processing and cleaning extracted content
4. Analyzing data with AI
5. Building interactive tools for end users
6. Handling real-world challenges (authentication, rate limits, errors)

Students gain practical skills applicable to:

* Research data collection
* Market analysis
* News monitoring
* Content aggregation
* Automated reporting
* Data journalism

---

### **IMPORTANT**

> "Web scraping combined with AI analysis opens up infinite possibilities for data collection and insights.
> The key is to start simple, iterate rapidly, and always respect ethical boundaries.
> Every website is a potential data source - your creativity is the only limit."
